{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quality-simpson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# External library\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Local library\n",
    "from dataset import DataSet\n",
    "import transformer as tfr\n",
    "import seq2seq as s2s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d520fd5c",
   "metadata": {},
   "source": [
    "# Prepare Translation DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "humanitarian-current",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file and create dataset\n",
    "data = DataSet(max_length=16)\n",
    "data.read_file('./data/fra.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdda9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample, sample_dec = data.tokenize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc700d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample['input_ids'][0])\n",
    "print(sample['labels'][0])\n",
    "print(sample_dec[0])\n",
    "print(data.tokenizer.decode(sample['labels'][0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68f3224",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensors = list()\n",
    "for i in tqdm(range(len(sample_dec))):\n",
    "    input_ids  = sample['input_ids'][i]\n",
    "    valid_lens = sample['attention_mask'][i].sum()\n",
    "    labels     = sample['labels'][i]\n",
    "    dec_inputs = sample_dec[i]\n",
    "    tensors.append((input_ids, dec_inputs, valid_lens, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc9efbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(tensors[:512], batch_size=128, shuffle=False)\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    enc_inputs, dec_inputs, valid_lens, labels = batch\n",
    "    print(enc_inputs.shape)\n",
    "    print(dec_inputs.shape)\n",
    "    print(valid_lens.shape)\n",
    "    print(labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89800b5",
   "metadata": {},
   "source": [
    "# Create NMT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "horizontal-rebecca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create transformer Seq2Seq model\n",
    "# input parameters of encoder and decoder\n",
    "# (vocab_size, num_hiddens, ffn_num_hiddens, num_heads, num_blks, dropout, use_bias=False)\n",
    "vocab_size  = data.vocab_size\n",
    "num_hiddens = 256\n",
    "ffn_hiddens = 64\n",
    "num_heads   = 4\n",
    "num_blks    = 2\n",
    "dropout     = 0.2\n",
    "\n",
    "# Use transformer encoder/decoder. Can also use GRU encoder/decoder\n",
    "encoder = tfr.TransformerEncoder(vocab_size, num_hiddens, ffn_hiddens, num_heads, num_blks, dropout)\n",
    "decoder = tfr.TransformerDecoder(vocab_size, num_hiddens, ffn_hiddens, num_heads, num_blks, dropout)\n",
    "\n",
    "# Seq2Seq model\n",
    "padding_index = data.tokenizer.pad_token_id\n",
    "lr = 1e-3\n",
    "\n",
    "model = s2s.Seq2Seq(encoder, decoder, padding_index, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e06cbe9",
   "metadata": {},
   "source": [
    "# Training our NMT models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba5e7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "print(device)\n",
    "\n",
    "epochs = 30\n",
    "\n",
    "# Use wandb to monitor the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad59024c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "losses = list()\n",
    "\n",
    "model.to(device)\n",
    "model.train()\n",
    "for epoch in trange(epochs):\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        a, b, c, d = batch\n",
    "        enc_inputs = a.to(device)\n",
    "        dec_inputs = b.to(device)\n",
    "        valid_lens = c.to(device)\n",
    "        labels     = d.to(device)\n",
    "        \n",
    "        Y_hat = model(enc_inputs, dec_inputs, valid_lens)\n",
    "        \n",
    "        loss = model.loss(Y_hat.transpose(1, 2), labels)\n",
    "        \n",
    "        model.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        model.optimizer.step()\n",
    "        \n",
    "        losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8550cf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bff850",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(tensors[:512], batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f035b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in test_dataloader:\n",
    "\n",
    "    preds, _ = model.predict_step(batch, device, data.max_length)\n",
    "    engs = [data.tokenizer.convert_ids_to_tokens(batch[0][i], skip_special_tokens=True) for i in range(4)]\n",
    "    fras = [data.tokenizer.convert_ids_to_tokens(batch[3][i], skip_special_tokens=True) for i in range(4)]\n",
    "    \n",
    "    for en, fr, p in zip(engs, fras, preds):\n",
    "        translation = []\n",
    "        for token in data.tokenizer.convert_ids_to_tokens(p):\n",
    "            if token == '</s>':\n",
    "                break\n",
    "            translation.append(token)\n",
    "        str_en = data.tokenizer.convert_tokens_to_string(en)\n",
    "        str_fr = data.tokenizer.convert_tokens_to_string(fr)\n",
    "        # print(translation)\n",
    "        print(f'{str_en} => {str_fr}, bleu,'\n",
    "              f'{s2s.bleu(translation, fr, k=2):.3f}')\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de53f560",
   "metadata": {},
   "outputs": [],
   "source": [
    "fras = [5682, 21, 2137, 19, 6381, 21, 682, 291, 0,]\n",
    "engs = [631, 250, 0, 59513]\n",
    "print(data.tokenizer.convert_tokens_to_string(data.tokenizer.convert_ids_to_tokens(engs)))\n",
    "print(data.tokenizer.convert_ids_to_tokens(fras))\n",
    "print(data.tokenizer.decode(fras))\n",
    "print(data.tokenizer.convert_tokens_to_string(data.tokenizer.convert_ids_to_tokens(fras)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7767ba3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2s.bleu(\"a b c d e\", \"a b c e f\", k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af37e8f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
