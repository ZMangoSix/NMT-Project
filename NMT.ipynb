{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "quality-simpson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# External library\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Local library\n",
    "from dataset import DataSet\n",
    "import transformer as tfr\n",
    "import seq2seq as s2s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d520fd5c",
   "metadata": {},
   "source": [
    "# Prepare Translation DataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328f47bc",
   "metadata": {},
   "source": [
    "## (1) Tatoeba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "humanitarian-current",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 229803\n"
     ]
    }
   ],
   "source": [
    "# Read file and create dataset\n",
    "data = DataSet(max_length=16)\n",
    "data.read_file('./data/fra.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b881c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample, sample_dec = data.tokenize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc700d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  631,   250,     0, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
      "        59513, 59513, 59513, 59513, 59513, 59513])\n",
      "tensor([  740,   291,     0, 59513, 59513, 59513, 59513, 59513, 59513, 59513,\n",
      "        59513, 59513, 59513, 59513, 59513, 59513])\n",
      "tensor([    0,   740,   291,     0, 59513, 59513, 59513, 59513, 59513, 59513,\n",
      "        59513, 59513, 59513, 59513, 59513, 59513])\n",
      "va!\n"
     ]
    }
   ],
   "source": [
    "print(sample['input_ids'][0])\n",
    "print(sample['labels'][0])\n",
    "print(sample_dec[0])\n",
    "print(data.tokenizer.decode(sample['labels'][0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803c4e9c",
   "metadata": {},
   "source": [
    "## (2) Ted talks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60d4e6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ted = DataSet(max_length=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20b8cdb4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataSet' object has no attribute 'read_xml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdata_ted\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_xml\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/IWSLT17.TED.tst2017.en-fr.en.xml\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataSet' object has no attribute 'read_xml'"
     ]
    }
   ],
   "source": [
    "data_ted.read_xml('./data/IWSLT17.TED.tst2017.en-fr.en.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69610880",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48ac298",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56e9a0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68f3224",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensors = list()\n",
    "for i in tqdm(range(len(sample_dec))):\n",
    "    input_ids  = sample['input_ids'][i]\n",
    "    valid_lens = sample['attention_mask'][i].sum()\n",
    "    labels     = sample['labels'][i]\n",
    "    dec_inputs = sample_dec[i]\n",
    "    tensors.append((input_ids, dec_inputs, valid_lens, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc9efbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(tensors[:512], batch_size=128, shuffle=False)\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    enc_inputs, dec_inputs, valid_lens, labels = batch\n",
    "    print(enc_inputs.shape)\n",
    "    print(dec_inputs.shape)\n",
    "    print(valid_lens.shape)\n",
    "    print(labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89800b5",
   "metadata": {},
   "source": [
    "# Create NMT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "horizontal-rebecca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create transformer Seq2Seq model\n",
    "# input parameters of encoder and decoder\n",
    "# (vocab_size, num_hiddens, ffn_num_hiddens, num_heads, num_blks, dropout, use_bias=False)\n",
    "vocab_size  = data.vocab_size\n",
    "num_hiddens = 256\n",
    "ffn_hiddens = 64\n",
    "num_heads   = 4\n",
    "num_blks    = 2\n",
    "dropout     = 0.2\n",
    "\n",
    "# Use transformer encoder/decoder. Can also use GRU encoder/decoder\n",
    "encoder = tfr.TransformerEncoder(vocab_size, num_hiddens, ffn_hiddens, num_heads, num_blks, dropout)\n",
    "decoder = tfr.TransformerDecoder(vocab_size, num_hiddens, ffn_hiddens, num_heads, num_blks, dropout)\n",
    "\n",
    "# Seq2Seq model\n",
    "padding_index = data.tokenizer.pad_token_id\n",
    "lr = 1e-3\n",
    "\n",
    "model = s2s.Seq2Seq(encoder, decoder, padding_index, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29b8fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create GRU Seq2Seq model\n",
    "vocab_size  = data.vocab_size\n",
    "embed_size = 256\n",
    "num_hiddens = 256\n",
    "num_layers = 2\n",
    "dropout = 0.2\n",
    "\n",
    "encoder = s2s.Seq2SeqEncoder(vocab_size, embed_size, num_hiddens, num_layers, dropout)\n",
    "decoder = s2s.Seq2SeqDecoder(vocab_size, embed_size, num_hiddens, num_layers, dropout)\n",
    "\n",
    "padding_index = data.tokenizer.pad_token_id\n",
    "lr = 1e-3\n",
    "\n",
    "model = s2s.Seq2Seq(encoder, decoder, padding_index, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e06cbe9",
   "metadata": {},
   "source": [
    "# Training our NMT models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba5e7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "print(device)\n",
    "\n",
    "epochs = 30\n",
    "\n",
    "# Use wandb to monitor the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad59024c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "losses = list()\n",
    "\n",
    "model.to(device)\n",
    "model.train()\n",
    "for epoch in trange(epochs):\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        a, b, c, d = batch\n",
    "        enc_inputs = a.to(device)\n",
    "        dec_inputs = b.to(device)\n",
    "        valid_lens = c.to(device)\n",
    "        labels     = d.to(device)\n",
    "        \n",
    "        Y_hat = model(enc_inputs, dec_inputs, valid_lens)\n",
    "        \n",
    "        loss = model.loss(Y_hat.transpose(1, 2), labels)\n",
    "        \n",
    "        model.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        model.optimizer.step()\n",
    "        \n",
    "        losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8550cf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65320273",
   "metadata": {},
   "source": [
    "# Create a SMT (Statistical Machine Translation) model as baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3ecd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import swadesh\n",
    "en2fr = [ (i.lower(), j.lower()) for i, j in swadesh.entries(['en', 'fr'])]\n",
    "translation_dict = dict(en2fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e701ca38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(sentence):\n",
    "    \"\"\"\n",
    "    Translate a sentence using the translation dictionary.\n",
    "    \n",
    "    Args:\n",
    "    sentence (str): Input sentence in English.\n",
    "    \n",
    "    Returns:\n",
    "    str: Translated sentence in French.\n",
    "    \"\"\"\n",
    "    # Tokenize the input sentence\n",
    "    tokens = sentence.lower().split()\n",
    "    \n",
    "    # Translate each token using the dictionary, if available\n",
    "    translated_tokens = [translation_dict.get(token, token) for token in tokens]\n",
    "    \n",
    "    # Join the translated tokens to form the translated sentence\n",
    "    translated_sentence = ' '.join(translated_tokens)\n",
    "    \n",
    "    return translated_sentence\n",
    "\n",
    "# Example usage\n",
    "english_sentence = \"far .\"\n",
    "french_translation = translate_sentence(english_sentence)\n",
    "print(\"English:\", english_sentence)\n",
    "print(\"French:\", french_translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f93fca5",
   "metadata": {},
   "source": [
    "# Evaluation of different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bff850",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(tensors[10000:10000+512], batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f035b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "for batch in test_dataloader:\n",
    "\n",
    "    preds, _ = model.predict_step(batch, device, data.max_length)\n",
    "    engs = [data.tokenizer.convert_ids_to_tokens(batch[0][i], skip_special_tokens=True) for i in range(4)]\n",
    "    fras = [data.tokenizer.convert_ids_to_tokens(batch[3][i], skip_special_tokens=True) for i in range(4)]\n",
    "    \n",
    "    for en, fr, p in zip(engs, fras, preds):\n",
    "        translation = []\n",
    "        for token in data.tokenizer.convert_ids_to_tokens(p):\n",
    "            if token == '</s>':\n",
    "                break\n",
    "            translation.append(token)\n",
    "        str_en = data.tokenizer.convert_tokens_to_string(en)\n",
    "        str_fr = data.tokenizer.convert_tokens_to_string(fr)\n",
    "        print(f'{str_en} => {str_fr}, bleu,'\n",
    "              f'{s2s.bleu(translation, fr, k=2):.3f}')\n",
    "        print(f'{str_en} => {translate_sentence(str_en)}, bleu,'\n",
    "              f'{s2s.bleu(translate_sentence(str_en), fr, k=2):.3f}')\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de53f560",
   "metadata": {},
   "outputs": [],
   "source": [
    "fras = [5682, 21, 2137, 19, 6381, 21, 682, 291, 0,]\n",
    "engs = [631, 250, 0, 59513]\n",
    "print(data.tokenizer.convert_tokens_to_string(data.tokenizer.convert_ids_to_tokens(engs)))\n",
    "print(data.tokenizer.convert_ids_to_tokens(fras))\n",
    "print(data.tokenizer.decode(fras))\n",
    "print(data.tokenizer.convert_tokens_to_string(data.tokenizer.convert_ids_to_tokens(fras)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7767ba3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2s.bleu(\"a b c d e\", \"a b c e f\", k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af37e8f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
