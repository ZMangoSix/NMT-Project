{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quality-simpson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For automatically reload import package\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Set Huggging Face Cache dir\n",
    "import os\n",
    "cache_dir = '/lustre/umt3/user/manyuan/CourseWork/huggingface'\n",
    "os.environ['HF_HOME'] = cache_dir\n",
    "\n",
    "# System library\n",
    "import random\n",
    "import gc\n",
    "\n",
    "# External library\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Local library\n",
    "from dataset import DataSet\n",
    "import transformer as tfr\n",
    "import seq2seq as s2s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d520fd5c",
   "metadata": {},
   "source": [
    "# Prepare Translation DataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328f47bc",
   "metadata": {},
   "source": [
    "## (1) Tatoeba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "humanitarian-current",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file and create dataset\n",
    "datafile = './data/fra.txt'\n",
    "data = DataSet(max_length=128, source='en', target='fr')\n",
    "data.read_file(datafile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b881c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample, sample_dec = data.tokenize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc700d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check specific sample at specific index\n",
    "sample_id = 0\n",
    "print(sample['input_ids'][sample_id])\n",
    "print(sample['labels'][sample_id])\n",
    "print(sample_dec[sample_id])\n",
    "print(data.tokenizer.decode(sample['labels'][sample_id], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803c4e9c",
   "metadata": {},
   "source": [
    "## (2) Ted talks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d4e6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ted = DataSet(max_length=128, source='en', target='fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b8cdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ted.read_xml('./data/IWSLT17.TED.tst2017.fr-en.fr.xml', './data/IWSLT17.TED.tst2017.en-fr.en.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69610880",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ted, sample_ted_dec = data_ted.tokenize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48ac298",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample_ted['input_ids'][0])\n",
    "print(sample_ted['labels'][0])\n",
    "print(sample_ted_dec[0])\n",
    "print(data_ted.tokenizer.decode(sample_ted['labels'][0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc02ceb",
   "metadata": {},
   "source": [
    "So now we can treat both dataset in the same way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f309b1a-40d9-4db8-9a06-21bbfc9958da",
   "metadata": {},
   "source": [
    "## Prepare Dataloader for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e1c91a-ec26-4bfa-8730-dc3a5b1a5c6f",
   "metadata": {},
   "source": [
    "## Train dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68f3224",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensors = list()\n",
    "for i in tqdm(range(len(sample_dec))):\n",
    "    input_ids  = sample['input_ids'][i]\n",
    "    valid_lens = sample['attention_mask'][i].sum()\n",
    "    labels     = sample['labels'][i]\n",
    "    dec_inputs = sample_dec[i]\n",
    "    tensors.append((input_ids, dec_inputs, valid_lens, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc9efbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_samples = len(tensors)\n",
    "split = int(num_train_samples*0.8)\n",
    "random.shuffle(tensors)\n",
    "\n",
    "train_dataloader = DataLoader(tensors[:split], batch_size=128, shuffle=True)\n",
    "dev_dataloader = DataLoader(tensors[split:], batch_size=8)\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    enc_inputs, dec_inputs, valid_lens, labels = batch\n",
    "    print(enc_inputs.shape)\n",
    "    print(dec_inputs.shape)\n",
    "    print(valid_lens.shape)\n",
    "    print(labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a987cf-ccee-49cc-a614-35585332837c",
   "metadata": {},
   "source": [
    "## Test dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50ffe63-d062-42f2-ad84-872ef274f36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensors = list()\n",
    "for i in tqdm(range(len(sample_ted_dec))):\n",
    "    input_ids  = sample_ted['input_ids'][i]\n",
    "    valid_lens = sample_ted['attention_mask'][i].sum()\n",
    "    labels     = sample_ted['labels'][i]\n",
    "    dec_inputs = sample_ted_dec[i]\n",
    "    tensors.append((input_ids, dec_inputs, valid_lens, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fee370-3186-4548-b14b-fc076193ca8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(tensors, batch_size=8)\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    enc_inputs, dec_inputs, valid_lens, labels = batch\n",
    "    print(enc_inputs.shape)\n",
    "    print(dec_inputs.shape)\n",
    "    print(valid_lens.shape)\n",
    "    print(labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89800b5",
   "metadata": {},
   "source": [
    "# Create NMT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "horizontal-rebecca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create transformer Seq2Seq model\n",
    "# input parameters of encoder and decoder\n",
    "# (vocab_size, num_hiddens, ffn_num_hiddens, num_heads, num_blks, dropout, use_bias=False)\n",
    "# vocab_size  = data_ted.vocab_size\n",
    "vocab_size  = data.vocab_size + 1 # add a bos token\n",
    "num_hiddens = 256\n",
    "ffn_hiddens = 64\n",
    "num_heads   = 4\n",
    "num_blks    = 2\n",
    "dropout     = 0.5\n",
    "\n",
    "# Use transformer encoder/decoder. Can also use GRU encoder/decoder\n",
    "encoder = tfr.TransformerEncoder(vocab_size, num_hiddens, ffn_hiddens, num_heads, num_blks, dropout)\n",
    "decoder = tfr.TransformerDecoder(vocab_size, num_hiddens, ffn_hiddens, num_heads, num_blks, dropout)\n",
    "\n",
    "# Seq2Seq model\n",
    "padding_index = data.tokenizer.pad_token_id\n",
    "# padding_index = data_ted.tokenizer.pad_token_id\n",
    "lr = 5e-4\n",
    "\n",
    "model = s2s.Seq2Seq(encoder, decoder, padding_index, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29b8fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create GRU Seq2Seq model\n",
    "# vocab_size  = data_ted.vocab_size\n",
    "vocab_size  = data.vocab_size + 1 # add a <bos> special token\n",
    "embed_size = 256\n",
    "num_hiddens = 256\n",
    "num_layers = 2\n",
    "dropout = 0.5\n",
    "\n",
    "encoder = s2s.Seq2SeqEncoder(vocab_size, embed_size, num_hiddens, num_layers, dropout)\n",
    "decoder = s2s.Seq2SeqDecoder(vocab_size, embed_size, num_hiddens, num_layers, dropout)\n",
    "\n",
    "# padding_index = data_ted.tokenizer.pad_token_id\n",
    "padding_index = data.tokenizer.pad_token_id\n",
    "lr = 5e-4\n",
    "\n",
    "model = s2s.Seq2Seq(encoder, decoder, padding_index, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e06cbe9",
   "metadata": {},
   "source": [
    "# Training our NMT models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba5e7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "print(device)\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "# Use wandb to monitor the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38f57e0-790c-49ff-a83f-bef1451432ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model from previous checkpoint\n",
    "model.load_state_dict(torch.load('models/transformer.pt'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8569f9-cfc6-4fc7-b21e-622e16ca8961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model from previous checkpoint\n",
    "model.load_state_dict(torch.load('models/gru.pt'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa44a21-df0f-403a-ab92-d6b7ff6d8f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get evaluation set loss\n",
    "def get_loss(model, device, dataloader):\n",
    "    dev_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            a, b, c, d = batch\n",
    "            enc_inputs = a.to(device)\n",
    "            dec_inputs = b.to(device)\n",
    "            valid_lens = c.to(device)\n",
    "            labels     = d.to(device)\n",
    "            \n",
    "            Y_hat = model(enc_inputs, dec_inputs, valid_lens)\n",
    "            \n",
    "            loss = model.loss(Y_hat.transpose(1, 2), labels)\n",
    "            dev_loss += loss.item()\n",
    "    \n",
    "    return dev_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad59024c",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = list()\n",
    "\n",
    "model.to(device)\n",
    "model.train()\n",
    "for epoch in trange(epochs):\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        a, b, c, d = batch\n",
    "        enc_inputs = a.to(device)\n",
    "        dec_inputs = b.to(device)\n",
    "        valid_lens = c.to(device)\n",
    "        labels     = d.to(device)\n",
    "        \n",
    "        Y_hat = model(enc_inputs, dec_inputs, valid_lens)\n",
    "        \n",
    "        loss = model.loss(Y_hat.transpose(1, 2), labels)\n",
    "        \n",
    "        model.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        model.optimizer.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "\n",
    "    if (epoch+1)%2 == 0:\n",
    "        torch.save(model.state_dict(), f'models/transformer_{epoch}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8550cf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.grid()\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Time')\n",
    "plt.savefig('transformer_loss.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea741cd-082b-4aca-82e8-8507a188ed2d",
   "metadata": {},
   "source": [
    "# Save checkpoint for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533f7318-55fe-437a-945a-ef9ad0ecd897",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'models/transformer.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a9d515-1152-4572-a9c7-070d27902e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'models/gru.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65320273",
   "metadata": {},
   "source": [
    "# Create a SMT (Statistical Machine Translation) model as baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3ecd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('swadesh')\n",
    "from nltk.corpus import swadesh\n",
    "en2fr = [ (i.lower(), j.lower()) for i, j in swadesh.entries(['en', 'fr'])]\n",
    "translation_dict = dict(en2fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e701ca38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(sentence):\n",
    "    \"\"\"\n",
    "    Translate a sentence using the translation dictionary.\n",
    "    \n",
    "    Args:\n",
    "    sentence (str): Input sentence in English.\n",
    "    \n",
    "    Returns:\n",
    "    str: Translated sentence in French.\n",
    "    \"\"\"\n",
    "    # Tokenize the input sentence\n",
    "    tokens = sentence.lower().split()\n",
    "    \n",
    "    # Translate each token using the dictionary, if available\n",
    "    translated_tokens = [translation_dict.get(token, token) for token in tokens]\n",
    "    \n",
    "    # Join the translated tokens to form the translated sentence\n",
    "    translated_sentence = ' '.join(translated_tokens)\n",
    "    \n",
    "    return translated_sentence\n",
    "\n",
    "# Example usage\n",
    "english_sentence = \"far .\"\n",
    "french_translation = translate_sentence(english_sentence)\n",
    "print(\"English:\", english_sentence)\n",
    "print(\"French:\", french_translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065ecce0-fab2-4298-9542-91ba9101154d",
   "metadata": {},
   "source": [
    "# Try pretrained Marian MT model (HuggingFace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3981d5-593d-4b44-8dd9-ca37b6320a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MarianTokenizer, MarianMTModel\n",
    "src = 'en'  # source language\n",
    "tgt = 'fr'  # target language\n",
    "sample_text = \"We are connecting.\"\n",
    "\n",
    "mname = f'Helsinki-NLP/opus-mt-{src}-{tgt}'\n",
    "model = MarianMTModel.from_pretrained(mname)\n",
    "tok = MarianTokenizer.from_pretrained(mname)\n",
    "batch = tok.prepare_seq2seq_batch(src_texts=[sample_text], return_tensors='pt')\n",
    "gen = model.generate(**batch)  # for forward pass: model(**batch)\n",
    "words = tok.batch_decode(gen, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f93fca5",
   "metadata": {},
   "source": [
    "# Evaluation of different models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4af434",
   "metadata": {},
   "source": [
    "## (1) BLEU and BERT Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b90acf-2736-4941-b674-6ccba735f577",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_smt(dataloader, batch_size, lang=\"fr\", is_RNN=False, batch_total=0):\n",
    "    total_bleu = 0\n",
    "    total_bertscore = 0\n",
    "    total_count = 0\n",
    "    batch_count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            total_count += len(batch[0])\n",
    "            batch_count += 1\n",
    "    \n",
    "            srcs = [data.tokenizer.convert_ids_to_tokens(batch[0][i], skip_special_tokens=True) for i in range(batch_size)]\n",
    "            tgts = [data.tokenizer.convert_ids_to_tokens(batch[3][i], skip_special_tokens=True) for i in range(batch_size)]\n",
    "            \n",
    "            for src, tgt in zip(srcs, tgts):\n",
    "                str_src = data.tokenizer.convert_tokens_to_string(src)\n",
    "                str_tgt = data.tokenizer.convert_tokens_to_string(tgt)\n",
    "        \n",
    "                if not batch_total:\n",
    "                    # SMT model\n",
    "                    print(translate_sentence(str_src))\n",
    "                    print(f'{str_src} => {translate_sentence(str_tgt)}, bleu, '\n",
    "                          f'{s2s.bleu(translate_sentence(str_src).split(\" \"), tgt, k=2):.3f}')\n",
    "        \n",
    "                    print(f'{str_src} => {translate_sentence(str_tgt)}, bleu, '\n",
    "                          f'{s2s.bert_score(translate_sentence(str_src), str_tgt, lang=lang)[\"f1\"][0]:.3f}')\n",
    "                else:\n",
    "                    total_bleu += s2s.bleu(translate_sentence(str_src).split(\" \"), tgt, k=2)\n",
    "                    total_bertscore += s2s.bert_score(translate_sentence(str_src), str_tgt, lang=lang)[\"f1\"][0]\n",
    "                \n",
    "                gc.collect()\n",
    "    \n",
    "            if batch_count >= batch_total:\n",
    "                break\n",
    "    \n",
    "    return total_bleu, total_bertscore, total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117b40b0-ae5f-4fc2-b7c4-03ae816d79f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_smt(test_dataloader, 8, lang=\"fr\", is_RNN=False, batch_total=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f035b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, device, dataloader, batch_size, beam_width=0, lang=\"fr\", is_RNN=False, batch_total=0):\n",
    "    total_bleu = 0\n",
    "    total_bertscore = 0\n",
    "    total_count = 0\n",
    "    batch_count = 0\n",
    "    \n",
    "    model.to(device)\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            total_count += len(batch[0])\n",
    "            batch_count += 1\n",
    "    \n",
    "            # Beam search decoding\n",
    "            if beam_width:\n",
    "                preds = model.beam_search(batch, device, beam_width, data.max_length, is_RNN)\n",
    "            # Greedy decoding\n",
    "            else:\n",
    "                preds, _ = model.predict_step(batch, device, data.max_length)\n",
    "            srcs = [data.tokenizer.convert_ids_to_tokens(batch[0][i], skip_special_tokens=True) for i in range(batch_size)]\n",
    "            tgts = [data.tokenizer.convert_ids_to_tokens(batch[3][i], skip_special_tokens=True) for i in range(batch_size)]\n",
    "            \n",
    "            for src, tgt, p in zip(srcs, tgts, preds):\n",
    "                translation = []\n",
    "                for token in data.tokenizer.convert_ids_to_tokens(p):\n",
    "                    if token == '</s>':\n",
    "                        break\n",
    "                    translation.append(token)\n",
    "                str_src = data.tokenizer.convert_tokens_to_string(src)\n",
    "                str_tgt = data.tokenizer.convert_tokens_to_string(tgt)\n",
    "                pred = data.tokenizer.convert_tokens_to_string(translation)\n",
    "        \n",
    "                if not batch_total:\n",
    "                    print(pred)\n",
    "                    # BLEU Score\n",
    "                    print(f'{str_src} => {str_tgt}, bleu, '\n",
    "                          f'{s2s.bleu(translation, tgt, k=2):.3f}')\n",
    "                    # BERT Score\n",
    "                    print(f'{str_src} => {str_tgt}, bert score, '\n",
    "                          f'{s2s.bert_score(pred, str_tgt, lang=lang)[\"f1\"][0]:.3f}')\n",
    "                else:\n",
    "                    total_bleu += s2s.bleu(translation, tgt, k=2)\n",
    "                    total_bertscore += s2s.bert_score(pred, str_tgt, lang=lang)[\"f1\"][0]\n",
    "                \n",
    "                gc.collect()\n",
    "    \n",
    "            if batch_count >= batch_total:\n",
    "                break\n",
    "        \n",
    "    return total_bleu, total_bertscore, total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e247402f-dbe3-4332-80a7-eb3ab7d7d395",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "evaluate(model, device, dev_dataloader, 8, 2, lang=\"fr\", is_RNN=False, batch_total=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0129eb78-9f86-4ae5-bddd-c2d540a555bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "evaluate(model, device, test_dataloader, 8, 2, lang=\"fr\", is_RNN=False, batch_total=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6e0b75-ff89-4762-9240-7f6104d7e02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "evaluate(model, device, dev_dataloader, 8, 0, lang=\"fr\", is_RNN=False, batch_total=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b718b5a4-d365-4660-a53a-3b29ae1a9a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "evaluate(model, device, test_dataloader, 8, 0, lang=\"fr\", is_RNN=False, batch_total=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de53f560",
   "metadata": {},
   "outputs": [],
   "source": [
    "fras = [5682, 21, 2137, 19, 6381, 21, 682, 291, 0,]\n",
    "engs = [631, 250, 0, 59513]\n",
    "print(data.tokenizer.convert_tokens_to_string(data.tokenizer.convert_ids_to_tokens(engs)))\n",
    "print(data.tokenizer.convert_ids_to_tokens(fras))\n",
    "print(data.tokenizer.decode(fras))\n",
    "print(data.tokenizer.convert_tokens_to_string(data.tokenizer.convert_ids_to_tokens(fras)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7767ba3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2s.bleu(\"a b c d e\", \"a b c e f\", k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a117a4f-9857-4c19-b8fe-1797a0bdcc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Random DEV (0.0, 98.9659451842308, 160)\n",
    "Random TEST (0.0, 103.15149623155594, 160)\n",
    "\n",
    "SMT DEV (0.0, 108.05254489183426, 160)\n",
    "SMT TEST (0.0, 114.14204689860344, 160)\n",
    "\n",
    "Transformer Beam   DEV (53.59059118759569, 142.55582463741302, 160)\n",
    "Transformer Greedy DEV (59.013246619166466, 143.2861720919609, 160)\n",
    "Transformer Beam   TEST (15.869870679217367, 133.20187187194824, 160)\n",
    "Transformer Greedy TEST (15.715572405590233, 132.49793833494186, 160)\n",
    "\n",
    "GRU Beam   DEV (79.33526214634732, 146.7177917957306, 160)\n",
    "GRU Greedy DEV (84.5888387368823, 147.6853220462799, 160)\n",
    "GRU Beam   TEST (15.335201627572879, 132.85215973854065, 160)\n",
    "GRU Greedy TEST (13.138322543843737, 132.49497658014297, 160)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
